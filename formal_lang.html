<h1 id="top">Formal Description Language</h1>
<p>
	Honestly, I was kinda mucking about, but I think I found a cozy little language (or should I say, algebra?) that can both describe data structures and programs.
	On its own, it doesn't talk about anything that hasn't already been talked in mathematics or information/computation/type theory, but I appreciate its symplicity,
	and, as always, have some fun ideas about a possible practical application.
</p>
<h2>Table of contents</h2>
<ul>
	<li><a href="#baseline">Baseline</a></li>
	<ul>
		<li><a href="#comma">Comma</a></li>
		<li><a href="#space">Space</a></li>
		<li><a href="#equality">Equality</a></li>
	</ul>
	<li><a href="#cardinality">Cardinality</a></li>
	<li><a href="#complex_operations">Defining more complicated operations</a></li>
	<ul>
		<li><a href="#logical_conjunction">Case study: Logical_Conjunction</a></li>
		<li><a href="#modular_addition">Case study: Modular Addition</a></li>
		<li><a href="#under_over_specification">Under- and Overspecification</a></li>
<!--		<li><a href="#binary_addition">Case study: Binary Addition</a></li>-->
	</ul>
<!--<li><a href="#"></a></li>-->
</ul>
<h2 id="baseline">Baseline</h2>
<p>
	It starts with a very basic set of rules. First, we define <code>:=</code>.
	It allows us to substitute the left side with the right side, whenever we encounter the left side in an expression.<br/>
	To give an example:
<pre><code> a := b;</code></pre>
	This means that, whenever an <code>a</code> is encountered, we can substitute it for a <code>b</code>,
	but not the other way around.
</p><p>
	Next, we define 3 different operators, that can be used to form expressions.
</p>

<h3 id="comma">Comma</h3>
<p>
	The first is <code>,</code>, a comma. A comma denotes a list of possible symbols that something can be.<br/>
	As an example:
<pre><code>b := F, T;
d := 0, 1, 2, 3, 4, 5, 6, 7, 8, 9;</code></pre>
	The <code>b</code> is a boolean. it can be either True or False.
	<code>d</code> is a decimal digit, it can be any symbol between 0 and 9.
	In type theory, this would be considered a 'sum type' (more on this later).
</p>

<h3 id="space">Space</h3>
<p>
	The second operator is <code> </code>, empty space. One might call it a stretch to call spaces 'operators',
	but they do denote a unique meaning: concatenation.<br/>
	Here, we define <code>u</code> to be two <code>b</code> symbols (booleans).
<pre><code>u := b b;</code></pre>
	Each boolean could be either true or false, so this <code>u</code> symbol can have 4 possible states.
	Based on this information, it might not be surprising that this is called a 'product type' in type theory (again, remember this for later).
</p>

<h3 id="equality">Equality</h3>
<p>
	The third operator is <code>=</code>, equality. It asserts that a certain symbol is equal to another.
	In the case of 'sum type'-symbols, this means that an equality expression specifies which sub-symbol that symbol embodies.<br/>
	The most basic case of this is in defining 'values' for 'constants', while maintaining their identity as an originally generalized 'type':
<pre><code>Y := b = T;</code></pre>
	In this usage, <code>Y</code> is a constant, <code>T</code> is a value, and <code>b</code> is, in essence, its type.
</p><p>
	There is a more interesting use of <code>=</code>, and that is that it gives us the ability to define other operations,
	by placing it on the left of a definition statement.
	For example, let's define <code>~</code>, the logic negation operator, for <code>b</code>:
<pre><code>~(b = T) := (b = F);
~(b = F) := (b = T);
</code></pre>
	Now, we immediately notice that I sneakily introduced parentheses, but those are purely 'syntactic sugar',
	to denote clear precedence. (We might prove at a later date that they can be notationed away.)<br>
	The next notable thing is the seeming redundancy of the notation. Why did I use <code>(b = T)</code> and the like,
	instead of simply:
<pre><code>~T := F;
~F := T;
</code></pre>
	It is because we were defining the <code>~</code> operator on <code>b</code>, and not on the underlying symbols.
	For a simple unary operation like negation, the difference of using the 'overarching' symbol does not matter,
	but on operations with more than one argument, and more complex types, the amount of possible interactions
	increases significantly. So if the operation allows for it, it would help if we could generalize, to cut down
	on amount of cases we have to write out. Before we start doing that, we have to lay out some important terms:
</p>

<h2 id="cardinality">Cardinality</h2>
<p>
	Recall earlier that we called things with the comma 'sum types' and things with spaces 'product types'.
	These names derive from how many states a symbol can be in.
	The earlier defined decimal digit <code>d</code> is practical to use as an example for this.
<pre><code>d := 0, 1, 2, 3, 4, 5, 6, 7, 8, 9;</code></pre>
	It can 'contain' any symbol from 0 to 9; 10 possible states. We will call this its <i>cardinality</i>,
	the amount of states a symbol or expression can be in. Booleans have a cardinality of 2.
</p><p>
	But where do the names 'sum type' and 'product type' come from? To explain this, let's look at the following symbol:
<pre><code>q := b, d;</code></pre>
	<code>q</code> can be <code>b</code> or <code>d</code>, <code>b</code> has a cardinality of 2, <code>d</code> has a cardinality of 10.
	Because <code>q</code> can be all the states of <code>b</code>, or all the states of <code>d</code>,
	<code>q</code> gets a cardinality of 12, which is the <i>sum</i> of the cardinality of the symbols it is made up from.<br/>
	Because this is not type theory, and we want to talk about more than just types,
	we will call symbols and expressions that have this property <i>sum-like</i>.
</p><p>
	For 'product type' (henceforth <i>product-like</i>) symbols and expressions, we can provide a very similar example:
<pre><code>p := b d;</code></pre>
	<code>p</code> contains both a <code>b</code> and <code>d</code>, so it's possible amount of states is the cardinality of <code>b</code> multiplied
	by the cardinality of <code>d</code>: 2×10 = 20. Indeed, the <i>product</i> of the consituant cardinalities.
</p><p>
	What about symbols like <code>T</code> or <code>5</code>, that we used without giving them their own definition?
	They simply have a cardinality of 1; they can only ever be themselves.
</p><p>
	These intuitions for the naming scheme are good, but there is one more feature of cardinality, particularly related to sum-like expressions,
	that somewhat strains the word 'sum'. The given definition of cardinality is "The amount of possible states of an expression."<br/>
	This directly means that finding the cardinality of an expression with overlapping states is more involved than just addition and multiplication.
	For example, let's consider the most basic case of 'overlapping states':
<pre><code>e := 3, 3;</code></pre>
	<code>e</code> has a cardinality of 1, because despite it having two symbols in the definition, its only possible state is <code>3</code>.
	The left and right <code>3</code> are indistinguishable. This also holds when the two symbols themselves have a larger cardinality:
<pre><code>q := d, d;</code></pre>
	This <code>q</code> has a cardinality of 10, despite both its elements also having a cardinality of 10, because they cover the same set of states.
	Partial overlap has the same effect as well:
<pre><code>g := 0, 1, 2;
h := 1, 2, 3;
f := g, h;
</code></pre>
	The cardinality of <code>f</code> is 4, its set of possible states is <code>0, 1, 2, 3</code>.
</p><p>
	This operation for combining sets so that they have no duplicate elements is called a <i>union</i>, and
	very strictly speaking, the cardinality of an expression is the cardinality of the <i>union</i> of
	the substituant possible states.<br/>
	This more formal definition has no effect on product-like expressions, but for sum-like expressions, it means an extra step
	has to be taken in calculating the cardinality.
</p>

<h2 id="complex_operations">Defining more complicated operations</h2>
<p>
	At the end of the <a href="#equality">chapter on Equality</a>, we briefly discussed how it might work in
	our favor to 'generalize' on operation definitions, and how it helps to use sum-like symbols for this.
</p>

<h3 id="logical_conjunction">Case study: Logical Conjunction</h3>
<p>
	As a first, 'low stakes' example, let's look at <code>&</code>, the logic conjunction operator.
	If we were to define it using purely <code>T</code> and <code>F</code>, it would look like this:
<pre><code>T & T := T;
F & T := F;
T & F := F;
F & F := F;
</code></pre>
	But, there is somewhat of a redundancy here. When either of the arguments is <code>F</code>,
	we know the result will also be <code>F</code>, regardless of the other argument. By using the <code>b</code>
	symbol, we can actually implement this generalization.<br/>
	We start by rewriting our first version to take booleans:
<pre><code>(b = T) & (b = T) := (b = T);
(b = F) & (b = T) := (b = F);
(b = T) & (b = F) := (b = F);
(b = F) & (b = F) := (b = F);
</code></pre>
	Then, we implement our generalization by omitting the equality check on one of the arguments if the other is <code>F</code>.
<pre><code>(b = T) & (b = T) := (b = T);
(b = F) & (b = T) := (b = F);
 b      & (b = F) := (b = F);
</code></pre>
	In doing this, you'll notice that the last case on the previous definition (<code>(b = F) & (b = F) := (b = F);</code>) has been generalized out.
	The definition now consists of three statements, instead of four.<br/>
	One could argue that these gains are marginal at best, so let's look at a more extreme case.
</p>

<h3 id="modular_addition">Case study: Modular Addition</h3>
<p>
	For our purposes, modular addition is when you take two digits, add them together normally, and then only take the least significant digit.
	So, for base 10, in modular addition, 5 + 5 = 0, 9 + 4 = 3 and 2 + 6 = 8.
</p><p>
	When we go to define this <code>+</code> operation, it's important to consider how many statements it would take to define it if we explicitly
	wrote down each case. In essence, the question we're really asking is: What is the cardinality of <code>d + d</code>?
	To answer it, we can use a nice emergent property of the fact that spaces define product-like expressions. Expressions like this, with operators
	in them, are also product-like. So, we can simply mulitiply the cardinalities of the substituant symbols to get an answer.
	<code>d</code> has a cardinality of 10, and <code>+</code>, since it's never defined in isolation, has a cardinality of 1.<br/>
	<code>d + d</code> therefore has a cardinality of 10×1×10 = 100. We'd have to write out 100 statements to fully define <code>+</code>
	if we went at it naively.
</p><p>
	Let's start with the 'identity' case. Adding 0 to a number leaves the number unchanged, so let's write that down.
<pre><code>(d = 0) + d := d;</code></pre>
	We immediately run into a problem. Which of the two <code>d</code>s is supposed to end up on the righthand side?
	There should be some sort of indication which one to use, since really the statement above doesn't actually say what we want it to.
</p><p>
	There are several ways to go about solving this, but for simplicity, we introduce another piece of useful syntactic sugar for
	brevity's sake. (Like parentheses, this too can be notationed away.)<br/>
	With an underscore (<code>_</code>), we can subscript parts of an expression and refer to them later on the righthand side of a definition statement.
<pre><code>(d_l = 0) + d_r := d_r;</code></pre>
	It's an underscore in ASCII like this, but on paper, we'd write it as a proper subscript for clarity.
	With this syntax, the part of the left of the underscore is evaluated as-is (in this example, just <code>d</code> for both), and the part on the right is only
	used for making a distinction.
</p><p>
	With this new syntax in the back of our heads, we can finalize our <code>+</code>. When working with this sort of arithmetic, it's common
	to define addition in terms of a 'Successor' function, which is just a very formal way to say "Rewrite everything as 1 + a lower number".
	The entire process of arriving at the complete set of definitions is somewhat beyond the scope of this article, but I'll write the result
	out in full, and then we can look at some interesting features later.
<pre><code>(d = 0) + d_r := d_r;

(d = 1) + (d = 0) := (d = 1);
(d = 1) + (d = 1) := (d = 2);
(d = 1) + (d = 2) := (d = 3);
(d = 1) + (d = 3) := (d = 4);
(d = 1) + (d = 4) := (d = 5);
(d = 1) + (d = 5) := (d = 6);
(d = 1) + (d = 6) := (d = 7);
(d = 1) + (d = 7) := (d = 8);
(d = 1) + (d = 8) := (d = 9);
(d = 1) + (d = 9) := (d = 0);

(d = 2) + d_r := (d = 1) + ((d = 1) + d_r);
(d = 3) + d_r := (d = 1) + ((d = 2) + d_r);
(d = 4) + d_r := (d = 1) + ((d = 3) + d_r);
(d = 5) + d_r := (d = 1) + ((d = 4) + d_r);
(d = 6) + d_r := (d = 1) + ((d = 5) + d_r);
(d = 7) + d_r := (d = 1) + ((d = 6) + d_r);
(d = 8) + d_r := (d = 1) + ((d = 7) + d_r);
(d = 9) + d_r := (d = 1) + ((d = 8) + d_r);
</code></pre>
	The total number of definitions is 19, a little under a fifth of the maximum possible 100. This method generalizes
	for every base (larger than base 2) and requires (2×base)-1 amount of definitions, whereas the total cardinality for this operation scales as base×base.
	What is the relation between these numbers? To properly understand this, we must understand what it means for an operation to
	be fully defined.
</p>

<h3 id="under_over_specification">Under- and Overspecification</h3>
<p>
	We will call the process of defining all cases of an operation <i>specifying</i>, and if we do our job properly, the operation will
	be <i>fully specified</i>. There are two elements to determining if an operation is fully specified. The first, we've discussed in the previous chapter,
	which is the insight that an expression (like <code>d + d</code>) has a cardinality (product-like, in the majority of cases). The second element of importance
	is slightly hinted at in the notation of statements, and the fact that they are seperated by semicolons.
</p><p>
	A set of statements is itself sum-like, though in a more nuanced sense. We can collect all the expressions on the left of the <code>:=</code>,
	and compute the cardinality of those expressions together as if they were sum-like. When we find this cardinality for a set of definitions that specify
	an operation it is called <i>the cardinality of the specification</i>. To see this in action, we'll repeat our definition of <code>&</code>.
<pre><code>(b = T) & (b = T) := (b = T);
(b = F) & (b = T) := (b = F);
 b      & (b = F) := (b = F);
</code></pre>
	We take all the expressions on the left side of the definitions:
<pre><code>(b = T) & (b = T), (b = F) & (b = T), b & (b = F)</code></pre>
	and compute the cardinality (take note that concatenation takes a higher precedence than the comma).
	There are a couple ways to go about this, but let's start by removing the <code>&</code>s, since they occur in every
	product-like part, and have a cardinality of 1.
<pre><code>(b = T) (b = T), (b = F) (b = T), b (b = F)</code></pre>
	Next, because we're just analyzing cardinality, we can omit all the <code>b = </code> as well.
<pre><code>T T, F T, b F</code></pre>
	At this point, we basically know the cardinality already, but for completeness sake, we should fully expand the expression.
<pre><code>T T, F T, (F, T) F</code></pre>
	The first and second items of this collection (<code>T T</code> and <code>F T</code>) have a cardinality of 1, and are not the
	same state, so we have at least a total cardinality of 2. Then the final item (<code>(F, T) F</code>) might be more
	difficult to immediately spot the cardinality of, but when we list the possible states that it could be in:
<pre><code>F F, T F</code></pre>
	We can see that that part's cardinality is also 2, and has no overlap with the first two items.
</p><p>
	So, the total cardinality of our specification of <code>&</code> is 4, which matches the cardinality of <code>b & b</code>,
	and therefore we know that it is fully specified.
</p><p>
	Now, we might be lazy, and instead (rather sloppily) write something like:
<pre><code>(b = T) & (b = T) := (b = T);
(b = F) &  b     := (b = F);
</code></pre>
	Later, after a good night's rest, we look at it again, notice something's off, and get to work computing the cardinality.
<pre><code>1:
	(b = T) & (b = T), (b = f) & b
2:
	T T, F b
3:
	T T, F (F, T)
4:
	T T, F F, F T
</code></pre>
	Ah, the cardinality is 3, even though we needed 4... This is <i>underspecification</i>: not all cases of the operation are specified.
</p><p>
	A quickthinking reader might already have considered what it would be like for this process to produce a cardinality greater than the
	fully specified cardinality. Sadly, the described process can never produce such a cardinality (unless we cheated (I will not elaborate (lest someone cheats))).
	The proof is left as an excercise to the reader for now.
</p><p>
	However, the title of this chapter still says what it does, so what else am I hiding? Suppose the following alternative specification of <code>&</code>
<pre><code>(b = T) & (b = T) := (b = T);
(b = F) &  b      := (b = F);
 b      & (b = F) := (b = F);
</code></pre>
	If we go through the process of finding the cardinality of it, we'll see something that might raise an eyebrow, but the result will be 4.
	A different process will be required to catch what's wrong with this specification. We do the same as earlier, taking the left sides
	of the statements and simplifying, but we'll keep them seperated:
<pre><code>1:
	(b = T) & (b = T)
	(b = F) &  b
	 b      & (b = F)
2:
	T T
	F b
	b F
3:
	T      T
	F      (F, T)
	(F, T) F
4:
	T T
	F F, F T
	F T, F F
</code></pre>
	Now, we take the cardinalities of these seperate expressions, and only then sum them. The first expression has a cardinality of 1, the second 2, and the third also 2.
	This different, <i>disjoint</i> cardinality gives a result of 5. That's more than 4. Indeed, the <code>F F</code> shows up twice in our specification: it is <i>overspecified</i>.
</p><p>
	Some more things of note about these attributes is that they are not mutually exclusive with eachother. In fact, an operation can even be both fully specified as well as overspecified,
	though it would not be <i>correctly</i> specified.
</p><p>
	A specification that is both underspecified and overspecified is:
<pre><code>(b = F) &  b      := (b = F);
 b      & (b = F) := (b = F);
</code></pre>
	To make matters worse, its disjoint cardinality is 4 as well. This is possible because overspecification happens when there are duplicate states in the specification.
	The correct way to detect this is by looking at the difference between the normal and disjoint cardinalities; if there is any, the specification is overspecified.
</p>

<!--

	When we carefully examine what is happening here, we might actually notice a property that both the previous 'optimized'
	lists of definitions share with eachother.
	Looking at all the left-side arguments for <code>+</code> (on the left side of <code>:=</code>), we see that every possible
	'sub-symbol' of <code>d</code> is addressed, from <code>0</code> to <code>9</code>.
	Looking at all the right-side arguments, we see the same.<br/>
	There is some sort of implied limit to the minimum amount of statements one needs to describe operations like this.

F (F, T), (F, T) F
F F, F T, F F, T F

<h3 id="binary_addition">Case study: Binary Addition</h3>
<p>
<pre><code>
b := 0, 1;
u := b b;
n := b b b b;

(b = 1) & (b = 1) := (b = 1);
(b = 0) &  b      := (b = 0);
 b      & (b = 0) := (b = 0);

 b      | (b = 1) := (b = 1);
(b = 1) |  b      := (b = 1);
(b = 0) | (b = 0) := (b = 0);

b_l + b_r := (u = (b_l | b_r) (b_l & b_r));

(u_l = b_0l b_1l) + (u_r = b_0r b_1r) := (


(n_l = b_0l b_1l b_2l b_3l) + (n_r = b_0r b_1r b_2r b_3r) := (b_0_l + b_0_r)

</code></pre>

</p>
-->

<!--
<pre><code></code></pre>

<p>
	Without immediate proof, I propose a theorem:<br/>
	If an operation with two arguments is an automorphism, the minimum amount of definition statements
	required for full coverage is bounded between C and 2×C, where C is the cardinality of the symbol it operates on.<br/>
</p>

-->
