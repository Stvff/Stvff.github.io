<h1 id="top">Formal Description Language</h1>
<p>
	Honestly, I was kinda mucking about, but I think I found a cozy little language (or should I say, algebra?) that can both describe data structures and programs.
	On its own, it doesn't talk about anything that hasn't already been talked in mathematics or information/computation/type theory, but I appreciate its symplicity,
	and, as always, have some fun ideas about a possible practical application.
</p>
<h2>Table of contents</h2>
<ul>
	<li><a href="#outline">Outline</a></li>
	<ul>
		<li><a href="#comma">Comma</a></li>
		<li><a href="#space">Space</a></li>
		<li><a href="#equality">Equality</a></li>
	</ul>
	<li><a href="#cardinality">Cardinality</a></li>
	<li><a href="#complex_operations">Defining more complicated operations</a></li>
	<ul>
		<li><a href="#logical_conjunction">Case study: Logical_Conjunction</a></li>
		<li><a href="#modular_addition">Case study: Modular Addition</a></li>
<!--		<li><a href="#binary_addition">Case study: Binary Addition</a></li>-->
	</ul>
<!--<li><a href="#"></a></li>-->
</ul>
<h2 id="outline">Outline</h2>
<p>
	It starts with a very basic set of rules. First, we define <code>:=</code>.
	It allows us to substitute the left side with the right side, whenever we encounter the left side in an expression.<br/>
	To give an example:
<pre><code> a := b;</code></pre>
	This means that, whenever an <code>a</code> is encountered, we can substitute it for a <code>b</code>,
	but not the other way around.
</p><p>
	Next, we define 3 different operators, that can be used to form expressions.
</p>

<h3 id="comma">Comma</h3>
<p>
	The first is <code>,</code>, a comma. A comma denotes a list of possible symbols that something can be.<br/>
	As an example:
<pre><code>b := F, T;
d := 0, 1, 2, 3, 4, 5, 6, 7, 8, 9;</code></pre>
	The <code>b</code> is a boolean. it can be either True or False.
	<code>d</code> is a decimal digit, it can be any symbol between 0 and 9.
	In type theory, this would be considered a 'sum type' (more on this later).
</p>

<h3 id="space">Space</h3>
<p>
	The second operator is <code> </code>, empty space. One might call it a stretch to call spaces 'operators',
	but they do denote a unique meaning: concatenation.<br/>
	Here, we define <code>u</code> to be two <code>b</code> symbols (booleans).
<pre><code>u := b b;</code></pre>
	Each boolean could be either true or false, so this <code>u</code> symbol can have 4 possible states.
	Based on this information, it might not be surprising that this is called a 'product type' in type theory (again, remember this for later).
</p>

<h3 id="equality">Equality</h3>
<p>
	The third operator is <code>=</code>, equality. It asserts that a certain symbol is equal to another.
	In the case of 'sum type'-symbols, this means that an equality expression specifies which sub-symbol that symbol embodies.<br/>
	The most basic case of this is in defining 'values' for 'constants', while maintaining their identity as an originally generalized 'type':
<pre><code>Y := b = T;</code></pre>
	In this usage, <code>Y</code> is a constant, <code>T</code> is a value, and <code>b</code> is, in essence, its type.
</p><p>
	There is a more interesting use of <code>=</code>, and that is that it gives us the ability to define other operations,
	by placing it on the left of a definition statement.
	For example, let's define <code>~</code>, the logic negation operator, for <code>b</code>:
<pre><code>~(b = T) := (b = F);
~(b = F) := (b = T);
</code></pre>
	Now, we immediately notice that I sneakily introduced parentheses, but those are purely 'syntactic sugar',
	to denote clear precedence. (We might prove at a later date that they can be notationed away.)<br>
	The next notable thing is the seeming redundancy of the notation. Why did I use <code>(b = T)</code> and the like,
	instead of simply:
<pre><code>~T := F;
~F := T;
</code></pre>
	It is because we were defining the <code>~</code> operator on <code>b</code>, and not on the underlying symbols.
	For a simple unary operation like negation, the difference of using the 'overarching' symbol does not matter,
	but on operations with more than one argument, and more complex types, the amount of possible interactions
	increases significantly. So if the operation allows for it, it would help if we could generalize, to cut down
	on amount of cases we have to write out. Before we start doing that, we have to lay out some important terms:
</p>

<h2 id="cardinality">Cardinality</h2>
<p>
	Recall earlier that we called things with the comma 'sum types' and things with spaces 'product types'.
	These names derive from how many states a symbol can be in.
	The earlier defined decimal digit <code>d</code> is practical to use as an example for this.
<pre><code>d := 0, 1, 2, 3, 4, 5, 6, 7, 8, 9;</code></pre>
	It can 'contain' any symbol from 0 to 9; 10 possible states. We will call this its <i>cardinality</i>,
	the amount of states a symbol or expression can be in. Booleans have a cardinality of 2.
</p><p>
	But where do the names 'sum type' and 'product type' come from? To explain this, let's look at the following symbol:
<pre><code>q := b, d;</code></pre>
	<code>q</code> can be <code>b</code> or <code>d</code>, <code>b</code> has a cardinality of 2, <code>d</code> has a cardinality of 10.
	Because <code>q</code> can be all the states of <code>b</code>, or all the states of <code>d</code>,
	<code>q</code> gets a cardinality of 12, which is the <i>sum</i> of the cardinality of the symbols it is made up from.<br/>
	Because this is not type theory, and we want to talk about more than just types,
	we will call symbols and expressions that have this property <i>sum-like</i>.
</p><p>
	For 'product type' (henceforth <i>product-like</i>) symbols and expressions, we can provide a very similar example:
<pre><code>p := b d;</code></pre>
	<code>p</code> contains both a <code>b</code> and <code>d</code>, so it's possible amount of states is the cardinality of <code>b</code> multiplied
	by the cardinality of <code>d</code>: 2×10 = 20. Indeed, the <i>product</i> of the consituant cardinalities.
</p><p>
	What about symbols like <code>T</code> or <code>5</code>, that we used without giving them their own definition?
	They simply have a cardinality of 1; they can only ever be themselves.
</p>

<h2 id="complex_operations">Defining more complicated operations</h2>
<p>
	At the end of the <a href="#equality">chapter on Equality</a>, we briefly discussed how it might work in
	our favor to 'generalize' on operation definitions, and how it helps to use sum-like symbols for this.
</p>

<h3 id="logical_conjunction">Case study: Logical Conjunction</h3>
<p>
	As a first, 'low stakes' example, let's look at <code>&</code>, the logic conjunction operator.
	If we were to define it using purely <code>T</code> and <code>F</code>, it would look like this:
<pre><code>T & T := T;
F & T := F;
T & F := F;
F & F := F;
</code></pre>
	But, there is somewhat of a redundancy here. When either of the arguments is <code>F</code>,
	we know the result will also be <code>F</code>, regardless of the other argument. By using the <code>b</code>
	symbol, we can actually implement this generalization.<br/>
	We start by rewriting our first version to take booleans:
<pre><code>(b = T) & (b = T) := (b = T);
(b = F) & (b = T) := (b = F);
(b = T) & (b = F) := (b = F);
(b = F) & (b = F) := (b = F);
</code></pre>
	Then, we implement our generalization by omitting the equality check on one of the arguments if the other is <code>F</code>.
<pre><code>(b = T) & (b = T) := (b = T);
(b = F) &  b      := (b = F);
 b      & (b = F) := (b = F);
</code></pre>
	In doing this, you'll notice that the last case on the previous definition (<code>(b = F) & (b = F) := (b = F);</code>) has been generalized out.
	The definition now consists of three statements, instead of four.<br/>
	One could argue that these gains are marginal at best, so let's look at a more extreme case.
</p>

<h3 id="modular_addition">Case study: Modular Addition</h3>
<p>
	For our purposes, modular addition is when you take two digits, add them together normally, and then only take the least significant digit.
	So, for base 10, in modular addition, 5 + 5 = 0, 9 + 4 = 3 and 2 + 6 = 8.
</p><p>
	When we go to define this <code>+</code> operation, it's important to consider how many statements it would take to define it if we explicitly
	wrote down each case. In essence, the question we're really asking is: What is the cardinality of <code>d + d</code>?
	To answer it, we can use a nice emergent property of the fact that spaces define product-like expressions. Expressions like this, with operators
	in them, are also product-like. So, we can simply mulitiply the cardinalities of the substituant symbols to get an answer.
	<code>d</code> has a cardinality of 10, and <code>+</code>, since it's never defined in isolation, has a cardinality of 1.<br/>
	<code>d + d</code> therefore has a cardinality of 10×1×10 = 100. We'd have to write out 100 statements to fully define <code>+</code>
	if we went at it naively.
</p><p>
	Let's start with the 'identity' case. Adding 0 to a number leaves the number unchanged, so let's write that down.
<pre><code>(d = 0) + d := d;</code></pre>
	We immediately run into a problem. Which of the two <code>d</code>s is supposed to end up on the righthand side?
	There should be some sort of indication which one to use, since really the statement above doesn't actually say what we want it to.
</p><p>
	There are several ways to go about solving this, but for simplicity, we introduce another piece of useful syntactic sugar for
	brevity's sake. (Like parentheses, this too can be notationed away.)<br/>
	With an underscore (<code>_</code>), you can subscript parts of an expression and refer to them later on the righthand side of a definition statement.
<pre><code>(d_l = 0) + d_r := d_r;</code></pre>
	It's an underscore in ASCII like this, but on paper, you'd write it as a proper subscript for clarity.
	With this syntax, the part of the left of the underscore is evaluated as-is (in this example, just <code>d</code> for both), and the part on the right is only
	used for making a distinction.
</p><p>
	With this new syntax in the back of our heads, we can finalize our <code>+</code>. When working with this sort of arithmetic, it's common
	to define addition in terms of a 'Successor' function, which is just a very formal way to say "Rewrite everything as 1 + a lower number".
	The actual full process of arriving at the full set of definitions is somewhat beyond the scope of this article, but I'll write it
	out in full, and then we can look at some interesting features.
<pre><code>(d = 0) + d_r := d_r;

(d = 1) + (d = 0) := (d = 1);
(d = 1) + (d = 1) := (d = 2);
(d = 1) + (d = 2) := (d = 3);
(d = 1) + (d = 3) := (d = 4);
(d = 1) + (d = 4) := (d = 5);
(d = 1) + (d = 5) := (d = 6);
(d = 1) + (d = 6) := (d = 7);
(d = 1) + (d = 7) := (d = 8);
(d = 1) + (d = 8) := (d = 9);
(d = 1) + (d = 9) := (d = 0);

(d = 2) + d_r := (d = 1) + ((d = 1) + d_r);
(d = 3) + d_r := (d = 1) + ((d = 2) + d_r);
(d = 4) + d_r := (d = 1) + ((d = 3) + d_r);
(d = 5) + d_r := (d = 1) + ((d = 4) + d_r);
(d = 6) + d_r := (d = 1) + ((d = 5) + d_r);
(d = 7) + d_r := (d = 1) + ((d = 6) + d_r);
(d = 8) + d_r := (d = 1) + ((d = 7) + d_r);
(d = 9) + d_r := (d = 1) + ((d = 8) + d_r);
</code></pre>
	The total number of definitions is 19, a little under a fifth of the maximum possible 100. This method generalizes
	for every base (larger than base 2) and requires (2×base)-1 amount of definitions, whereas cardinality increases with base×base.
</p><p>
	When we carefully examine what is happening here, we might actually notice a property that this 'optimized'
	list of definitions shares with the 'optimized' list of definitions
	for <code>&</code> (defined right before this chapter).<br/>
	Looking at all the left-side arguments for <code>+</code> (on the left side of <code>:=</code>), we see that every possible
	'sub-symbol' of <code>d</code> is addressed, from <code>0</code> to <code>9</code>.
	Looking at all the right-side arguments, we see the same.<br/>
	There is some sort of implied limit to the minimum amount of statements one needs to describe operations like this,
	but let's not think too hard about that yet.
</p>

<!--
<h3 id="binary_addition">Case study: Binary Addition</h3>
<p>
<pre><code>
b := 0, 1;
u := b b;
n := b b b b;

(b = 1) & (b = 1) := (b = 1);
(b = 0) &  b      := (b = 0);
 b      & (b = 0) := (b = 0);

 b      | (b = 1) := (b = 1);
(b = 1) |  b      := (b = 1);
(b = 0) | (b = 0) := (b = 0);

b_l + b_r := (u = (b_l | b_r) (b_l & b_r));

(u_l = b_0l b_1l) + (u_r = b_0r b_1r) := (


(n_l = b_0l b_1l b_2l b_3l) + (n_r = b_0r b_1r b_2r b_3r) := (b_0_l + b_0_r)

</code></pre>

</p>
-->

<!--
<pre><code></code></pre>

<p>
	Without immediate proof, I propose a theorem:<br/>
	If an operation with two arguments is an automorphism, the minimum amount of definition statements
	required for full coverage is bounded between C and 2×C, where C is the cardinality of the symbol it operates on.<br/>
</p>

-->
